{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoModelForPreTraining, TrainingArguments, Trainer\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame with columns \"filename\" and \"emotion\"\n",
    "# data = pd.read_csv(\"C:/MyDocs/DTU/MSc/Thesis/Data/MELD/MELD_preprocess_test/pre_process_test.csv\")\n",
    "data = pd.read_csv(\"C:/Users/DANIEL/Desktop/thesis/low-resource-emotion-recognition/MELD_preprocess_test/pre_process_test.csv\")\n",
    "\n",
    "# directory = \"C:/MyDocs/DTU/MSc/Thesis/Data/MELD/MELD_preprocess_test/MELD_preprocess_test_data\"\n",
    "directory = \"C:/Users/DANIEL/Desktop/thesis/low-resource-emotion-recognition/MELD_preprocess_test/MELD_preprocess_test_data\"\n",
    "\n",
    "files = []\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith('.wav'):\n",
    "        files.append(file)\n",
    "\n",
    "# Add filenames to a new column in the DataFrame\n",
    "data['filename'] = files\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "raw_labels = data['Emotion'].values\n",
    "labels = label_encoder.fit_transform(raw_labels)\n",
    "\n",
    "# Show the label-encoding pairs:\n",
    "print(label_encoder.classes_)\n",
    "print(\"[0,         1,       2,       3,         4,         5]\")\n",
    "\n",
    "print(labels)\n",
    "\n",
    "max_length = 16000 * 10  # 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "\n",
    "    # Load audio file\n",
    "    file_to_load = row['filename']\n",
    "    file_to_load_path = os.path.join(directory, file_to_load)\n",
    "    # print()\n",
    "    # print(index)\n",
    "    # print(file_to_load)\n",
    "    # print()\n",
    "\n",
    "    audio, sr = librosa.load(file_to_load_path, sr=16000)\n",
    "    \n",
    "    if len(audio) > max_length:\n",
    "        audio = audio[:max_length]\n",
    "    else:\n",
    "        padding = max_length - len(audio)\n",
    "        offset = padding // 2\n",
    "        audio = np.pad(audio, (offset, padding - offset), 'constant')\n",
    "    \n",
    "    # Extract features (e.g., MFCCs)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    \n",
    "    # Mean across time\n",
    "    mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "    \n",
    "    features.append(mfccs_processed)\n",
    "    \n",
    "    # Encode label\n",
    "    # labels.append(label_encoder.transform([row['Emotion']]))\n",
    "\n",
    "# Convert to arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels).flatten()\n",
    "\n",
    "\n",
    "# Now, `features` and `labels` can be used for training your model\n",
    "# Optionally, save them to disk\n",
    "# np.save('features.npy', features)\n",
    "# np.save('labels.npy', labels)\n",
    "\n",
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "# Convert features and labels into PyTorch tensors\n",
    "features_tensor = torch.tensor(features).float()\n",
    "labels_tensor = torch.tensor(labels).long()  # Use .long() for integer labels, .float() for one-hot\n",
    "\n",
    "# Choose train indices and validation indices\n",
    "train_indices = np.random.choice(len(features), int(0.8 * len(features)), replace=False)\n",
    "val_indices = np.array([i for i in range(len(features)) if i not in train_indices])\n",
    "\n",
    "\n",
    "# Create dataset and dataloader for both training and validation sets\n",
    "train_dataset = TensorDataset(features_tensor[train_indices], labels_tensor[train_indices])\n",
    "val_dataset = TensorDataset(features_tensor[val_indices], labels_tensor[val_indices])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load a pre-trained model for pretrained\n",
    "model = AutoModelForPreTraining.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\", num_labels=5)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")\n",
    "\n",
    "# Initialize the trainer\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'emotion_recognition_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
