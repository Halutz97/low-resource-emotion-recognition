{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DANIEL\\anaconda3\\envs\\pfas\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import Wav2Vec2ForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'disgust' 'joy' 'neutral' 'sadness' 'surprise']\n",
      "[0,         1,       2,       3,         4,         5]\n",
      "[4 5 3 2 4 3 3 2 3 5 3 3 5 0 3 2 3 3 3 5 5 5 2 3 3 2 5 3 3 5 3 3 3 0 3 3 5\n",
      " 3 5 5 5 3 0 5 0 0 0 0 0 0 0 0 0 0 0 2 3 3 3 5 4 3 3 5 5 2 2 3 3 5 2 5 0 1\n",
      " 2 2 3 3 3 3 3 3 3 2 3 3 3 3 3 3 2 3 4 3 2 3 3 3 3 3]\n",
      "(100, 160000)\n",
      "(100,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-large-xlsr-53\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_attn_dim\": null,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.075,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame with columns \"filename\" and \"emotion\"\n",
    "# data = pd.read_csv(\"C:/MyDocs/DTU/MSc/Thesis/Data/MELD/MELD_preprocess_test/pre_process_test.csv\")\n",
    "data = pd.read_csv(\"C:/Users/DANIEL/Desktop/thesis/low-resource-emotion-recognition/MELD_preprocess_test/pre_process_test.csv\")\n",
    "\n",
    "# directory = \"C:/MyDocs/DTU/MSc/Thesis/Data/MELD/MELD_preprocess_test/MELD_preprocess_test_data\"\n",
    "directory = \"C:/Users/DANIEL/Desktop/thesis/low-resource-emotion-recognition/MELD_preprocess_test/MELD_preprocess_test_data\"\n",
    "\n",
    "files = []\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith('.wav'):\n",
    "        files.append(file)\n",
    "\n",
    "# Add filenames to a new column in the DataFrame\n",
    "data['filename'] = files\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "raw_labels = data['Emotion'].values\n",
    "labels = label_encoder.fit_transform(raw_labels)\n",
    "\n",
    "# Show the label-encoding pairs:\n",
    "print(label_encoder.classes_)\n",
    "print(\"[0,         1,       2,       3,         4,         5,       6]\")\n",
    "\n",
    "print(labels)\n",
    "\n",
    "max_length = 16000 * 6  # 10 seconds\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "\n",
    "    # Load audio file\n",
    "    file_to_load = row['filename']\n",
    "    file_to_load_path = os.path.join(directory, file_to_load)\n",
    "    # print()\n",
    "    # print(index)\n",
    "    # print(file_to_load)\n",
    "    # print()\n",
    "\n",
    "    audio, sr = librosa.load(file_to_load_path, sr=16000)\n",
    "    \n",
    "    if len(audio) > max_length:\n",
    "        audio = audio[:max_length]\n",
    "    else:\n",
    "        padding = max_length - len(audio)\n",
    "        offset = padding // 2\n",
    "        audio = np.pad(audio, (offset, padding - offset), 'constant')\n",
    "    \n",
    "    \n",
    "    features.append(audio)\n",
    "    \n",
    "    # Encode label\n",
    "    # labels.append(label_encoder.transform([row['Emotion']]))\n",
    "\n",
    "# Convert to arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels).flatten()\n",
    "\n",
    "\n",
    "# Now, `features` and `labels` can be used for training your model\n",
    "# Optionally, save them to disk\n",
    "# np.save('features.npy', features)\n",
    "# np.save('labels.npy', labels)\n",
    "\n",
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "# Convert features to a float tensor and transpose the last two dimensions\n",
    "features_tensor = torch.tensor(features).float()\n",
    "labels_tensor = torch.tensor(labels).long()  # Use .long() for integer labels, .float() for one-hot\n",
    "\n",
    "# Choose train indices and validation indices\n",
    "train_indices = np.random.choice(len(features), int(0.8 * len(features)), replace=False)\n",
    "val_indices = np.array([i for i in range(len(features)) if i not in train_indices])\n",
    "\n",
    "\n",
    "# Convert the TensorDatasets to Datasets\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_values': features_tensor[train_indices],\n",
    "    'labels': labels_tensor[train_indices]\n",
    "})\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'input_values': features_tensor[val_indices],\n",
    "    'labels': labels_tensor[val_indices]\n",
    "})\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load a pre-trained model for pretrained\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\", num_labels=7)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "# Initialize the trainer\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "print(model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DANIEL\\anaconda3\\envs\\pfas\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 33%|███▎      | 10/30 [23:39<44:40, 134.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7677257061004639, 'eval_accuracy': 0.5, 'eval_runtime': 55.3417, 'eval_samples_per_second': 0.361, 'eval_steps_per_second': 0.054, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 67%|██████▋   | 20/30 [48:06<25:00, 150.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7516752481460571, 'eval_accuracy': 0.5, 'eval_runtime': 61.4393, 'eval_samples_per_second': 0.326, 'eval_steps_per_second': 0.049, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "100%|██████████| 30/30 [1:16:31<00:00, 153.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7456499338150024, 'eval_accuracy': 0.5, 'eval_runtime': 55.8138, 'eval_samples_per_second': 0.358, 'eval_steps_per_second': 0.054, 'epoch': 3.0}\n",
      "{'train_runtime': 4591.0244, 'train_samples_per_second': 0.052, 'train_steps_per_second': 0.007, 'train_loss': 1.7601076761881511, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Prepare the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'emotion_recognition_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
